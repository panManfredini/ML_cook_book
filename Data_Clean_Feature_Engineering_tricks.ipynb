{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "Can be done in a few ways:\n",
    "- pd.getdummies(...)  for Dataframe\n",
    "- sklearn.preprocessing.OneHotEncoder for np Array\n",
    "- sklearn.preprocessing.LabelEncoder  for target only\n",
    "\n",
    "**Note**: many of these have a feature called **drop=\"first\"** this is to fix the problem that now there is one variable that is fully correlated with others values and unnecessary. The number of degrees of freedom are (N_categories-1).\n",
    "Example:\n",
    "\n",
    "|Var_A_green|Var_A_red| Var_A_yellow|\n",
    "-------------------|---------|----------\n",
    "|0|1|0|\n",
    "|1|0|0|\n",
    "|0|0|1|\n",
    "\n",
    "The fact that the categories are mutially exclusive means that if is \"green\", cannot be \"red\" or \"yellow\", so two of them are enough to describe the system.\n",
    "\n",
    "**drop=\"first\"** must be used when doing **fits**.\n",
    "\n",
    "### sparse matrix\n",
    "Since the one-hot encoded new colums are mutually exclusive they can be represented via the number of column that contains the \"1\" all the other will be zero. So one can represent it as (col_ON, col_tot) tuple. This representation has much lowe memory footprint, and called \"Sparse-Matrix\". Used for natural language, where immagine if you need one-hot encode the words in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
